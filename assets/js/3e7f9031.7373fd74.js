"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[119],{5680(e,n,a){a.d(n,{xA:()=>g,yg:()=>c});var t=a(6540);function l(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function r(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),a.push.apply(a,t)}return a}function i(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?r(Object(a),!0).forEach(function(n){l(e,n,a[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))})}return e}function o(e,n){if(null==e)return{};var a,t,l=function(e,n){if(null==e)return{};var a,t,l={},r=Object.keys(e);for(t=0;t<r.length;t++)a=r[t],n.indexOf(a)>=0||(l[a]=e[a]);return l}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(t=0;t<r.length;t++)a=r[t],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(l[a]=e[a])}return l}var s=t.createContext({}),p=function(e){var n=t.useContext(s),a=n;return e&&(a="function"==typeof e?e(n):i(i({},n),e)),a},g=function(e){var n=p(e.components);return t.createElement(s.Provider,{value:n},e.children)},d={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},m=t.forwardRef(function(e,n){var a=e.components,l=e.mdxType,r=e.originalType,s=e.parentName,g=o(e,["components","mdxType","originalType","parentName"]),m=p(a),c=l,u=m["".concat(s,".").concat(c)]||m[c]||d[c]||r;return a?t.createElement(u,i(i({ref:n},g),{},{components:a})):t.createElement(u,i({ref:n},g))});function c(e,n){var a=arguments,l=n&&n.mdxType;if("string"==typeof e||l){var r=a.length,i=new Array(r);i[0]=m;var o={};for(var s in n)hasOwnProperty.call(n,s)&&(o[s]=n[s]);o.originalType=e,o.mdxType="string"==typeof e?e:l,i[1]=o;for(var p=2;p<r;p++)i[p]=a[p];return t.createElement.apply(null,i)}return t.createElement.apply(null,a)}m.displayName="MDXCreateElement"},7400(e,n,a){a.r(n),a.d(n,{assets:()=>s,contentTitle:()=>i,default:()=>d,frontMatter:()=>r,metadata:()=>o,toc:()=>p});var t=a(8168),l=(a(6540),a(5680));const r={sidebar_position:7},i="Results and Comparison",o={unversionedId:"results",id:"results",title:"Results and Comparison",description:"Performance benchmarks and comparative analysis of the Classical ML and Deep Learning models.",source:"@site/docs/results.md",sourceDirName:".",slug:"/results",permalink:"/sentiment-production-pipeline/docs/results",draft:!1,editUrl:"https://github.com/Nouman64-cat/sentiment-production-pipeline/tree/main/docs/docs/results.md",tags:[],version:"current",sidebarPosition:7,frontMatter:{sidebar_position:7},sidebar:"tutorialSidebar",previous:{title:"Model Choices",permalink:"/sentiment-production-pipeline/docs/model-choices"},next:{title:"API Usage",permalink:"/sentiment-production-pipeline/docs/api-usage"}},s={},p=[{value:"Test Methodology",id:"test-methodology",level:2},{value:"Performance Summary",id:"performance-summary",level:2},{value:"Detailed Metrics",id:"detailed-metrics",level:2},{value:"Classical ML Model",id:"classical-ml-model",level:3},{value:"Deep Learning Model",id:"deep-learning-model",level:3},{value:"Speed vs Accuracy Trade-off",id:"speed-vs-accuracy-trade-off",level:2},{value:"Confusion Matrices",id:"confusion-matrices",level:2},{value:"Classical ML Model",id:"classical-ml-model-1",level:3},{value:"Deep Learning Model",id:"deep-learning-model-1",level:3},{value:"Cost Analysis",id:"cost-analysis",level:2},{value:"Inference Cost per 1M Requests",id:"inference-cost-per-1m-requests",level:3},{value:"Model Storage",id:"model-storage",level:3},{value:"When to Use Each Model",id:"when-to-use-each-model",level:2},{value:"Use Classical ML When:",id:"use-classical-ml-when",level:3},{value:"Use Deep Learning When:",id:"use-deep-learning-when",level:3},{value:"Hybrid Approach",id:"hybrid-approach",level:2},{value:"Running Your Own Benchmarks",id:"running-your-own-benchmarks",level:2},{value:"Conclusion",id:"conclusion",level:2},{value:"Recommendation",id:"recommendation",level:3},{value:"Future Improvements",id:"future-improvements",level:3}],g={toc:p};function d({components:e,...n}){return(0,l.yg)("wrapper",(0,t.A)({},g,n,{components:e,mdxType:"MDXLayout"}),(0,l.yg)("h1",{id:"results-and-comparison"},"Results and Comparison"),(0,l.yg)("p",null,"Performance benchmarks and comparative analysis of the Classical ML and Deep Learning models."),(0,l.yg)("h2",{id:"test-methodology"},"Test Methodology"),(0,l.yg)("p",null,"Both models were evaluated on an identical held-out test set:"),(0,l.yg)("ul",null,(0,l.yg)("li",{parentName:"ul"},(0,l.yg)("strong",{parentName:"li"},"Split"),": 10% of total data (80% train / 10% validation / 10% test)"),(0,l.yg)("li",{parentName:"ul"},(0,l.yg)("strong",{parentName:"li"},"Random State"),": 42 (reproducible)"),(0,l.yg)("li",{parentName:"ul"},(0,l.yg)("strong",{parentName:"li"},"Metrics"),": Accuracy, F1 Score, Precision, Recall"),(0,l.yg)("li",{parentName:"ul"},(0,l.yg)("strong",{parentName:"li"},"Hardware"),": Apple M-series / CPU (no dedicated GPU)")),(0,l.yg)("h2",{id:"performance-summary"},"Performance Summary"),(0,l.yg)("table",null,(0,l.yg)("thead",{parentName:"table"},(0,l.yg)("tr",{parentName:"thead"},(0,l.yg)("th",{parentName:"tr",align:null},"Metric"),(0,l.yg)("th",{parentName:"tr",align:null},"Classical ML (LogReg)"),(0,l.yg)("th",{parentName:"tr",align:null},"Deep Learning (DistilBERT)"))),(0,l.yg)("tbody",{parentName:"table"},(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},(0,l.yg)("strong",{parentName:"td"},"Accuracy")),(0,l.yg)("td",{parentName:"tr",align:null},"~87%"),(0,l.yg)("td",{parentName:"tr",align:null},"~92%")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},(0,l.yg)("strong",{parentName:"td"},"F1 Score")),(0,l.yg)("td",{parentName:"tr",align:null},"~0.86"),(0,l.yg)("td",{parentName:"tr",align:null},"~0.91")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},(0,l.yg)("strong",{parentName:"td"},"Inference Speed")),(0,l.yg)("td",{parentName:"tr",align:null},"~0.1 ms/sample"),(0,l.yg)("td",{parentName:"tr",align:null},"~15 ms/sample")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},(0,l.yg)("strong",{parentName:"td"},"Model Size")),(0,l.yg)("td",{parentName:"tr",align:null},"~1 MB"),(0,l.yg)("td",{parentName:"tr",align:null},"~260 MB")))),(0,l.yg)("admonition",{title:"Note",type:"info"},(0,l.yg)("p",{parentName:"admonition"},"Actual values depend on your specific dataset and training run. Run the analysis notebook for precise metrics.")),(0,l.yg)("h2",{id:"detailed-metrics"},"Detailed Metrics"),(0,l.yg)("h3",{id:"classical-ml-model"},"Classical ML Model"),(0,l.yg)("pre",null,(0,l.yg)("code",{parentName:"pre"},"              precision    recall  f1-score   support\n\n    negative       0.86      0.87      0.86       XXX\n    positive       0.87      0.86      0.87       XXX\n\n    accuracy                           0.87       XXX\n   macro avg       0.87      0.87      0.87       XXX\nweighted avg       0.87      0.87      0.87       XXX\n")),(0,l.yg)("h3",{id:"deep-learning-model"},"Deep Learning Model"),(0,l.yg)("pre",null,(0,l.yg)("code",{parentName:"pre"},"              precision    recall  f1-score   support\n\n    negative       0.91      0.92      0.91       XXX\n    positive       0.92      0.91      0.91       XXX\n\n    accuracy                           0.91       XXX\n   macro avg       0.91      0.91      0.91       XXX\nweighted avg       0.91      0.91      0.91       XXX\n")),(0,l.yg)("h2",{id:"speed-vs-accuracy-trade-off"},"Speed vs Accuracy Trade-off"),(0,l.yg)("pre",null,(0,l.yg)("code",{parentName:"pre"},"    F1 Score\n       \u2502\n  0.92 \u2524                    \u25cf DistilBERT\n       \u2502\n  0.90 \u2524\n       \u2502\n  0.88 \u2524\n       \u2502\n  0.86 \u2524    \u25cf Logistic Regression\n       \u2502\n  0.84 \u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       0    5    10   15   20   25   30\n                Inference Time (ms)\n")),(0,l.yg)("p",null,"The graph shows the classic speed-accuracy trade-off:"),(0,l.yg)("ul",null,(0,l.yg)("li",{parentName:"ul"},(0,l.yg)("strong",{parentName:"li"},"Logistic Regression"),": Near-instant inference, slightly lower accuracy"),(0,l.yg)("li",{parentName:"ul"},(0,l.yg)("strong",{parentName:"li"},"DistilBERT"),": Higher accuracy, 150x slower inference")),(0,l.yg)("h2",{id:"confusion-matrices"},"Confusion Matrices"),(0,l.yg)("h3",{id:"classical-ml-model-1"},"Classical ML Model"),(0,l.yg)("pre",null,(0,l.yg)("code",{parentName:"pre"},"                 Predicted\n              Neg    Pos\nActual  Neg   XXX    XX\n        Pos   XX     XXX\n")),(0,l.yg)("h3",{id:"deep-learning-model-1"},"Deep Learning Model"),(0,l.yg)("pre",null,(0,l.yg)("code",{parentName:"pre"},"                 Predicted\n              Neg    Pos\nActual  Neg   XXX    XX\n        Pos   XX     XXX\n")),(0,l.yg)("h2",{id:"cost-analysis"},"Cost Analysis"),(0,l.yg)("h3",{id:"inference-cost-per-1m-requests"},"Inference Cost per 1M Requests"),(0,l.yg)("table",null,(0,l.yg)("thead",{parentName:"table"},(0,l.yg)("tr",{parentName:"thead"},(0,l.yg)("th",{parentName:"tr",align:null},"Factor"),(0,l.yg)("th",{parentName:"tr",align:null},"Classical ML"),(0,l.yg)("th",{parentName:"tr",align:null},"DistilBERT"))),(0,l.yg)("tbody",{parentName:"table"},(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"Compute Time"),(0,l.yg)("td",{parentName:"tr",align:null},"~2 CPU-minutes"),(0,l.yg)("td",{parentName:"tr",align:null},"~4 CPU-hours")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"AWS Lambda Cost","*"),(0,l.yg)("td",{parentName:"tr",align:null},"~$0.02"),(0,l.yg)("td",{parentName:"tr",align:null},"~$15.00")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"Memory Required"),(0,l.yg)("td",{parentName:"tr",align:null},"128 MB"),(0,l.yg)("td",{parentName:"tr",align:null},"512+ MB")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"Cold Start"),(0,l.yg)("td",{parentName:"tr",align:null},"~100ms"),(0,l.yg)("td",{parentName:"tr",align:null},"~5 seconds")))),(0,l.yg)("p",null,"*","Estimated based on Lambda pricing at $0.0000166667 per GB-second"),(0,l.yg)("h3",{id:"model-storage"},"Model Storage"),(0,l.yg)("table",null,(0,l.yg)("thead",{parentName:"table"},(0,l.yg)("tr",{parentName:"thead"},(0,l.yg)("th",{parentName:"tr",align:null},"Model"),(0,l.yg)("th",{parentName:"tr",align:null},"Size"),(0,l.yg)("th",{parentName:"tr",align:null},"S3 Storage/Month","*"))),(0,l.yg)("tbody",{parentName:"table"},(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"ML Model"),(0,l.yg)("td",{parentName:"tr",align:null},"1 MB"),(0,l.yg)("td",{parentName:"tr",align:null},"$0.02")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"DL Model"),(0,l.yg)("td",{parentName:"tr",align:null},"260 MB"),(0,l.yg)("td",{parentName:"tr",align:null},"$6.00")))),(0,l.yg)("p",null,"*","Standard S3 pricing"),(0,l.yg)("h2",{id:"when-to-use-each-model"},"When to Use Each Model"),(0,l.yg)("h3",{id:"use-classical-ml-when"},"Use Classical ML When:"),(0,l.yg)("ul",null,(0,l.yg)("li",{parentName:"ul"},"\u2705 Latency SLA < 50ms"),(0,l.yg)("li",{parentName:"ul"},"\u2705 High request volume (>1000 RPS)"),(0,l.yg)("li",{parentName:"ul"},"\u2705 Cost-sensitive deployment"),(0,l.yg)("li",{parentName:"ul"},"\u2705 Simple positive/negative classification"),(0,l.yg)("li",{parentName:"ul"},"\u2705 Edge or serverless deployment")),(0,l.yg)("h3",{id:"use-deep-learning-when"},"Use Deep Learning When:"),(0,l.yg)("ul",null,(0,l.yg)("li",{parentName:"ul"},"\u2705 Accuracy is paramount"),(0,l.yg)("li",{parentName:"ul"},"\u2705 Batch processing (overnight jobs)"),(0,l.yg)("li",{parentName:"ul"},"\u2705 Nuanced sentiment analysis needed"),(0,l.yg)("li",{parentName:"ul"},"\u2705 Premium tier feature"),(0,l.yg)("li",{parentName:"ul"},"\u2705 Human-in-the-loop fallback")),(0,l.yg)("h2",{id:"hybrid-approach"},"Hybrid Approach"),(0,l.yg)("p",null,"For production, consider a hybrid strategy:"),(0,l.yg)("pre",null,(0,l.yg)("code",{parentName:"pre",className:"language-python"},'def predict_with_fallback(text, ml_model, dl_model, threshold=0.7):\n    # Fast path: Use ML model\n    ml_proba = ml_model.predict_proba([text])[0]\n    confidence = max(ml_proba)\n\n    if confidence > threshold:\n        # High confidence: Use ML prediction\n        return "ml", ml_model.predict([text])[0]\n    else:\n        # Low confidence: Fall back to DL\n        return "dl", dl_model.predict(text)\n')),(0,l.yg)("p",null,"This approach:"),(0,l.yg)("ul",null,(0,l.yg)("li",{parentName:"ul"},"Handles 80%+ of requests with fast ML model"),(0,l.yg)("li",{parentName:"ul"},"Escalates ambiguous cases to DL model"),(0,l.yg)("li",{parentName:"ul"},"Balances speed and accuracy")),(0,l.yg)("h2",{id:"running-your-own-benchmarks"},"Running Your Own Benchmarks"),(0,l.yg)("p",null,"Use the analysis notebook for custom benchmarks:"),(0,l.yg)("pre",null,(0,l.yg)("code",{parentName:"pre",className:"language-bash"},"cd notebooks\njupyter notebook analysis.ipynb\n")),(0,l.yg)("p",null,"The notebook produces:"),(0,l.yg)("ol",null,(0,l.yg)("li",{parentName:"ol"},"Side-by-side accuracy comparison"),(0,l.yg)("li",{parentName:"ol"},"Confusion matrix visualizations"),(0,l.yg)("li",{parentName:"ol"},"Inference time measurements"),(0,l.yg)("li",{parentName:"ol"},"Model size comparison")),(0,l.yg)("h2",{id:"conclusion"},"Conclusion"),(0,l.yg)("h3",{id:"recommendation"},"Recommendation"),(0,l.yg)("p",null,(0,l.yg)("strong",{parentName:"p"},"For initial production deployment, use the Classical ML Model.")),(0,l.yg)("p",null,(0,l.yg)("strong",{parentName:"p"},"Reasoning:")),(0,l.yg)("ol",null,(0,l.yg)("li",{parentName:"ol"},(0,l.yg)("strong",{parentName:"li"},"Speed"),": ~150x faster than transformer model"),(0,l.yg)("li",{parentName:"ol"},(0,l.yg)("strong",{parentName:"li"},"Cost"),": 100x cheaper at scale"),(0,l.yg)("li",{parentName:"ol"},(0,l.yg)("strong",{parentName:"li"},"Size"),": 260x smaller, easier to deploy"),(0,l.yg)("li",{parentName:"ol"},(0,l.yg)("strong",{parentName:"li"},"Accuracy Gap"),": Only ~5% difference, often acceptable")),(0,l.yg)("p",null,(0,l.yg)("strong",{parentName:"p"},"Reserve DistilBERT for:")),(0,l.yg)("ul",null,(0,l.yg)("li",{parentName:"ul"},"Premium/enterprise tier"),(0,l.yg)("li",{parentName:"ul"},"Offline batch processing"),(0,l.yg)("li",{parentName:"ul"},"Cases where ML model shows low confidence")),(0,l.yg)("h3",{id:"future-improvements"},"Future Improvements"),(0,l.yg)("ol",null,(0,l.yg)("li",{parentName:"ol"},(0,l.yg)("strong",{parentName:"li"},"Ensemble Methods"),": Combine both models"),(0,l.yg)("li",{parentName:"ol"},(0,l.yg)("strong",{parentName:"li"},"Model Distillation"),": Train smaller DL model from DistilBERT"),(0,l.yg)("li",{parentName:"ol"},(0,l.yg)("strong",{parentName:"li"},"Quantization"),": Reduce DL model size by 4x"),(0,l.yg)("li",{parentName:"ol"},(0,l.yg)("strong",{parentName:"li"},"ONNX Export"),": Faster DL inference runtime")))}d.isMDXComponent=!0}}]);