"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[581],{5680(e,t,n){n.d(t,{xA:()=>p,yg:()=>d});var a=n(6540);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter(function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable})),n.push.apply(n,a)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach(function(t){r(e,t,n[t])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach(function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))})}return e}function o(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),c=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},p=function(e){var t=c(e.components);return a.createElement(s.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},g=a.forwardRef(function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,p=o(e,["components","mdxType","originalType","parentName"]),g=c(n),d=r,m=g["".concat(s,".").concat(d)]||g[d]||u[d]||i;return n?a.createElement(m,l(l({ref:t},p),{},{components:n})):a.createElement(m,l({ref:t},p))});function d(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,l=new Array(i);l[0]=g;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o.mdxType="string"==typeof e?e:r,l[1]=o;for(var c=2;c<i;c++)l[c]=n[c];return a.createElement.apply(null,l)}return a.createElement.apply(null,n)}g.displayName="MDXCreateElement"},7846(e,t,n){n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>l,default:()=>u,frontMatter:()=>i,metadata:()=>o,toc:()=>c});var a=n(8168),r=(n(6540),n(5680));const i={sidebar_position:7},l="Evaluation & Metrics",o={unversionedId:"evaluation",id:"evaluation",title:"Evaluation & Metrics",description:"This project uses a centralized evaluation module to ensure that all models are benchmarked using the exact same logic.",source:"@site/docs/evaluation.md",sourceDirName:".",slug:"/evaluation",permalink:"/sentiment-production-pipeline/docs/evaluation",draft:!1,editUrl:"https://github.com/Nouman64-cat/sentiment-production-pipeline/tree/main/docs/docs/evaluation.md",tags:[],version:"current",sidebarPosition:7,frontMatter:{sidebar_position:7}},s={},c=[{value:"Metrics Used",id:"metrics-used",level:2},{value:"Running Evaluations",id:"running-evaluations",level:2},{value:"1. Evaluate Classical ML Model",id:"1-evaluate-classical-ml-model",level:3},{value:"2. Evaluate Deep Learning Model",id:"2-evaluate-deep-learning-model",level:3},{value:"3. View in MLflow (Docker / Local)",id:"3-view-in-mlflow-docker--local",level:3},{value:"Code Functionality",id:"code-functionality",level:2},{value:"Key Dependencies",id:"key-dependencies",level:2}],p={toc:c};function u({components:e,...t}){return(0,r.yg)("wrapper",(0,a.A)({},p,t,{components:e,mdxType:"MDXLayout"}),(0,r.yg)("h1",{id:"evaluation--metrics"},"Evaluation & Metrics"),(0,r.yg)("p",null,"This project uses a centralized evaluation module to ensure that all models are benchmarked using the exact same logic."),(0,r.yg)("h2",{id:"metrics-used"},"Metrics Used"),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:"left"},"Metric"),(0,r.yg)("th",{parentName:"tr",align:"left"},"Description"),(0,r.yg)("th",{parentName:"tr",align:"left"},"Purpose"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:"left"},(0,r.yg)("strong",{parentName:"td"},"Accuracy")),(0,r.yg)("td",{parentName:"tr",align:"left"},"Fraction of correct predictions."),(0,r.yg)("td",{parentName:"tr",align:"left"},"General performance indicator.")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:"left"},(0,r.yg)("strong",{parentName:"td"},"F1 Score")),(0,r.yg)("td",{parentName:"tr",align:"left"},"Harmonic mean of precision and recall."),(0,r.yg)("td",{parentName:"tr",align:"left"},"Better than accuracy for imbalanced datasets (e.g., if we had very few negative reviews).")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:"left"},(0,r.yg)("strong",{parentName:"td"},"Confusion Matrix")),(0,r.yg)("td",{parentName:"tr",align:"left"},"Visualizes partial errors (False Positives vs False Negatives)."),(0,r.yg)("td",{parentName:"tr",align:"left"},"Helps understand ",(0,r.yg)("em",{parentName:"td"},"how")," the model is failing.")))),(0,r.yg)("h2",{id:"running-evaluations"},"Running Evaluations"),(0,r.yg)("p",null,"You can evaluate trained models independently using the ",(0,r.yg)("inlineCode",{parentName:"p"},"src/scripts/evaluate.py")," script. This script loads the model artifacts and the test portion of the dataset (defined in ",(0,r.yg)("inlineCode",{parentName:"p"},"config/*.yaml"),") to verify performance."),(0,r.yg)("h3",{id:"1-evaluate-classical-ml-model"},"1. Evaluate Classical ML Model"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},"uv run python -m src.scripts.evaluate --model ml\n")),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Output:")),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"Prints Accuracy and F1 Score to console."),(0,r.yg)("li",{parentName:"ul"},"Saves confusion matrix plot to ",(0,r.yg)("inlineCode",{parentName:"li"},"models/ml_confusion_matrix.png"),".")),(0,r.yg)("h3",{id:"2-evaluate-deep-learning-model"},"2. Evaluate Deep Learning Model"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},"uv run python -m src.scripts.evaluate --model dl\n")),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Output:")),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"Prints Accuracy and F1 Score to console."),(0,r.yg)("li",{parentName:"ul"},"Saves confusion matrix plot to ",(0,r.yg)("inlineCode",{parentName:"li"},"models/dl_confusion_matrix.png"),".")),(0,r.yg)("h3",{id:"3-view-in-mlflow-docker--local"},"3. View in MLflow (Docker / Local)"),(0,r.yg)("p",null,"If you are running the pipeline via ",(0,r.yg)("inlineCode",{parentName:"p"},"docker compose"),", you can view these metrics and the confusion matrix in the MLflow UI:"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},"Open ",(0,r.yg)("a",{parentName:"li",href:"http://localhost:5001"},"http://localhost:5001"),"."),(0,r.yg)("li",{parentName:"ol"},"Select your experiment (",(0,r.yg)("inlineCode",{parentName:"li"},"Sentiment_Analysis_ML")," or ",(0,r.yg)("inlineCode",{parentName:"li"},"Sentiment_Analysis_DL"),")."),(0,r.yg)("li",{parentName:"ol"},"Click on a run to see ",(0,r.yg)("strong",{parentName:"li"},"Metrics")," (Accuracy, F1)."),(0,r.yg)("li",{parentName:"ol"},"Scroll to ",(0,r.yg)("strong",{parentName:"li"},"Artifacts")," to view the ",(0,r.yg)("inlineCode",{parentName:"li"},"confusion_matrix.png"),".")),(0,r.yg)("h2",{id:"code-functionality"},"Code Functionality"),(0,r.yg)("p",null,"The evaluation logic is decoupled into:"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},"Definitions (",(0,r.yg)("inlineCode",{parentName:"strong"},"src/evaluation/metrics.py"),")"),":"),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},"Pure functions that take ",(0,r.yg)("inlineCode",{parentName:"li"},"y_true")," and ",(0,r.yg)("inlineCode",{parentName:"li"},"y_pred")," arrays."),(0,r.yg)("li",{parentName:"ul"},"Used by ",(0,r.yg)("em",{parentName:"li"},"both")," the training loop (for real-time logs) and the evaluation script (for final reports)."))),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},"Runner (",(0,r.yg)("inlineCode",{parentName:"strong"},"src/scripts/evaluate.py"),")"),":"),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},"Handles data loading, model instantiation, and batch inference."),(0,r.yg)("li",{parentName:"ul"},"Ensures the test set split is identical to training by reading ",(0,r.yg)("inlineCode",{parentName:"li"},"src/config/"),".")))),(0,r.yg)("h2",{id:"key-dependencies"},"Key Dependencies"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"scikit-learn"),": For metric calculations."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"matplotlib")," & ",(0,r.yg)("inlineCode",{parentName:"li"},"seaborn"),": For generating confusion matrix plots.")))}u.isMDXComponent=!0}}]);